{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3a0d022-56cf-4efa-9b5b-59ed65a36bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2 - DataFrame Relational Operations in Spark\n",
    "\n",
    "This demonstration shows how to effectively use joins and set operations with DataFrames, focusing on performance optimization and best practices.\n",
    "\n",
    "### Objectives\n",
    "- Understand different types of DataFrame joins\n",
    "- Implement performance optimizations for joins\n",
    "- Handle complex join scenarios\n",
    "- Use set operations effectively\n",
    "- Apply best practices for data skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8edad983-bc38-41a4-b6e7-7cfc27833b97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "465f1ecd-e301-41c7-b046-d691398484d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Setup and Data Loading\n",
    "\n",
    "First, let's load our sample retail data tables and examine their structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8127942f-106d-41f7-9449-40ef6ece7f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read the data \n",
    "transactions_df = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "customers_df = spark.read.table(\"samples.bakehouse.sales_customers\")\n",
    "franchises_df = spark.read.table(\"samples.bakehouse.sales_franchises\")\n",
    "suppliers_df = spark.read.table(\"samples.bakehouse.sales_suppliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c5bef41-859f-4519-a9ef-f2dc87630dbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Examine schemas for each of the dataframes\n",
    "transactions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13709882-3c8a-4c55-ab04-d854e550af9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b9dd29b-fd69-40d6-89c7-42339d42f04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "franchises_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5f70a43-bbf2-4cd1-873c-13004bf90d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "suppliers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4598a7a-96fe-4993-b5d5-0033ac239f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Basic Join Operations\n",
    "\n",
    "Let's start with simple join operations to combine our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "672e0ba4-3e98-4455-bef7-50fd1f1d1f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Python Inner join example to enrich the transactions with store information\n",
    "enriched_transactions = franchises_df.join(\n",
    "    transactions_df,\n",
    "    on=\"franchiseID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "display(enriched_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d67c05d2-e3d7-4776-8861-2a46a69649db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. The \"on\" clause can contain an expression\n",
    "enriched_transactions = franchises_df.join(\n",
    "    transactions_df,\n",
    "    on= transactions_df.franchiseID == franchises_df.franchiseID,\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "display(enriched_transactions)\n",
    "\n",
    "# This is particularly useful if the join key is named differently in both entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e43fd39a-b922-40b0-8d0c-f716a6b9dfa5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 5.  Create temporary views\n",
    "customers_df.createOrReplaceTempView(\"customers\")\n",
    "franchises_df.createOrReplaceTempView(\"franchises\")\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ee8c4f-a313-431b-9e6d-ad6afc945ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 2. SQL Inner join example to enrich the transactions with store information\n",
    "-- Spark temporary views allows the query to run in RAM memory which is much faster than disk memory\n",
    "\n",
    "SELECT *\n",
    "from transactions t\n",
    "INNER JOIN franchises f\n",
    "on t.franchiseID = f.franchiseID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f300213-c098-4111-bf51-6ed7b0e15dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Please note how all fields from both dataframes are present in the result, a better practice is to project the columns you need from each entity\n",
    "# We will also alias some of the columns to disambiguate column names\n",
    "enriched_transactions = franchises_df \\\n",
    "    .select(\n",
    "        \"franchiseID\", \n",
    "        col(\"name\").alias(\"store_name\"), \n",
    "        col(\"city\").alias(\"store_city\"), \n",
    "        col(\"country\").alias(\"store_country\")\n",
    "        ) \\\n",
    "    .join(\n",
    "        transactions_df,\n",
    "        on=\"franchiseID\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    \n",
    "display(enriched_transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "444b5142-c6bf-4c1c-a706-a8799848e0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- # 3. Please note how all fields from both dataframes are present in the result, a better practice is to project the columns you need from each entity\n",
    "-- # We will also alias some of the columns to disambiguate column names\n",
    "\n",
    "SELECT \n",
    "    f.franchiseID,\n",
    "    f.name as store_name,\n",
    "    f.city as store_city,\n",
    "    f.country as store_country\n",
    "FROM franchises f\n",
    "INNER JOIN transactions t\n",
    "ON f.franchiseID = t.franchiseID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "392f20ec-8a49-4ae6-9194-3244d3b67a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Create two separate INNER Join one in Python and another in SQL with that pulls the following data:\n",
    "1. Franchise country equal to US\n",
    "2. Group by franchiseID, name, city, country, paymentMethod\n",
    "3. Sum totalPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e749695-3770-4977-9911-04fab4d63a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- fillin SQL code below\n",
    "SELECT \n",
    "    sum(t.totalPrice) as total_revenue\n",
    "group by f.franchiseID, f.name, f.city, f.country, t.paymentMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbc851ca-601c-4de5-9d5a-4a751f60037c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Fillin Python code below\n",
    "\n",
    ").filter(col(\"country\") == \"US\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5️⃣  Group & aggregate\n",
    "# ------------------------------------------------------------------\n",
    "# Fillin Python code below\n",
    "agg = (\n",
    "    \n",
    "    .agg(spark_sum(\"totalPrice\").alias(\"total_revenue\"))   # sum(t.totalPrice)\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 7️⃣  Inspect / export the final DataFrame\n",
    "# ------------------------------------------------------------------\n",
    "display(agg)               # quick look\n",
    "# agg.write.csv(\"us_store_revenue.csv\", header=True)   # or any other export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb223c8-1aa2-43ae-9d2a-d359d5d14d2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Join Strategy**\n",
    "   - Use inner joins where keys exist in all dataframes\n",
    "   - Use outer joins where there is a possibility that keys don't exist in both dataframes\n",
    "   - Handle column name conflicts\n",
    "\n",
    "2. **Performance Optimization**\n",
    "   - Filter before joining\n",
    "   - Project only needed columns\n",
    "   - Handle skewed keys appropriately\n",
    "   - Reference the smaller dataframe first; or\n",
    "   - Use broadcast joins for small tables"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2878129512601890,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Working with Complex Data Types in Spark Exercise 2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
