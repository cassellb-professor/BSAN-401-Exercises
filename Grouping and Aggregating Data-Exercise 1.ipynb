{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "760835c0-c472-4557-bb51-0556b7aa03cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Grouping and Aggregating E-Commerce Data\n",
    "\n",
    "In this Exercise, you'll perform grouping and aggregation operations in Spark using bakerhouse transaction data. You'll perform basic grouping, multiple aggregations, and window functions.\n",
    "\n",
    "### Objectives\n",
    "- Use groupBy operations in Spark to summarize the data\n",
    "- Implement multiple aggregations\n",
    "- Apply different ordering functions and techniques\n",
    "- (Bonus) Use Windows function for advanced analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "232a7685-20fc-40c7-82b8-2b950b1c42a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "    - In the drop-down, select **More**.\n",
    "\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f29d00cf-5b08-4e7f-97c3-254a0dea4a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Data Setup and Loading\n",
    "\n",
    "First, load the retail transaction data and examine its structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f306f62-26eb-4ad3-b4f4-7a025e227936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "##1. Read and display the e-commerce transaction data\n",
    "## Read and displaying the e-commerce transaction data\n",
    "transactions_df = spark.read.table(\"samples.bakehouse.sales_transactions\")\n",
    "\n",
    "## 2. display a sample of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e46b5360-7c26-4fb7-90fa-a1e2b01afb6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "## B. Basic Grouping Operations\n",
    "\n",
    "Let's start with simple grouping operations to understand trip patterns by location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31d95522-2962-4197-97d2-03f3976ed728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 1. Group the data by products and count the number of sales\n",
    "\n",
    "## 2. display a product_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2980c86a-6067-4498-894e-4c1ea70ad8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Combining Multiple Aggregations\n",
    "\n",
    "Let's perform multiple aggregations by location using the `agg()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57fb0b16-f949-489b-bb15-f24ace7ce825",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## 1. Analyze sales by payment method\n",
    "## 2. Calculate the total revenue, average transaction value, and count of transactions for payment method\n",
    "## 3. Order by total revenue (highest first) \n",
    "\n",
    "# # Perform multiple aggregations by location, order by most popular pickup locations\n",
    "\n",
    "## 4. display a payment_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd6656b3-4e8d-4a61-8673-fa36807e7fba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Window Functions\n",
    "\n",
    "Now let's use window functions for more advanced analytics.\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdfbcc6e-8427-4acb-b7fb-058c78175bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Using window functions to add rankings\n",
    "## Ranking products by total revenue\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "## First, calculate total revenue by product and which comes from from pyspark.sql.functions import in A.\n",
    "product_revenue_df = transactions_df \\\n",
    "    .groupBy(\"product\") \\\n",
    "    .agg(\n",
    "        round(sum(col(\"totalPrice\")), 2).alias(\"total_revenue\")\n",
    "    )\n",
    "\n",
    "## 1. Create window spec for ranking categories\n",
    "\n",
    "\n",
    "## 2. Add rankings\n",
    "\n",
    "\n",
    "## 3. Display the rankings\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Grouping and Aggregating Data-Exercise 1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
